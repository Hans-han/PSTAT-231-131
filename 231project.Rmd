---
title: "project 231"
author: "Zongyi Han"
date: "2022-12-5"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

## Introduction
The purpose of this project is to predict Gold spot price as it can be used as a measurement of chaosness in the world. The project use supervised  model. 
Porject consisted of two part:
1.explore relationship between variables and perforem necessary adjustment
2.fit model and forecast

## What is GLD?
GLD is a ETF listed on NYSE, it tracked gold spot price closely.

## Why this project can be usful?
It can give directional advise on price movement and people may infer from the gold price to trade or make judgement on economic strength(Gold price inversely relates with real rate).

## 1.0 Data collection and cleaning
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

```{r load library ,class.source = 'fold-hide'}
library(tidymodels)
library(discrim)
library(poissonreg)
library(corrr)
tidymodels_prefer()
suppressMessages(library(tidyquant))
library(ggplot2)
library(fitdistrplus)
library(reshape2)
library(patchwork)
library(parallel)
```



```{r}
gld <- tq_get("gld") %>% 
       na.omit() %>% 
       tq_transmute(select = close,
                    mutate_fun = to.monthly,
                    col_rename = "gld.monthly",
                    indexAt="firstof")
head(gld)# here is the data
  #length(gld)
```

```{r}
UR <- tq_get("UNRATE", get = "economic.data") %>% 
    na.omit() %>% 
    tq_transmute(mutate_fun = to.period,
                 period     = "months", 
                 col_rename = "unemployment.rate",
                 indexAt="firstof")
head(UR)
```

```{r}
us10y <- tq_get("DGS10", get = "economic.data") %>% 
    na.omit() %>% 
    tq_transmute(mutate_fun = to.period,
                 period     = "months", 
                 col_rename = "US10.yield",
                 indexAt="firstof")
head(us10y)
```

```{r}
total_asset <- tq_get("WALCL", get = "economic.data") %>% 
    na.omit() %>% 
    tq_transmute(mutate_fun = to.period,
                 period     = "months", 
                 col_rename = "total.asset",
                 indexAt="firstof")
head(total_asset)
```

```{r}
real_rate <- tq_get("REAINTRATREARAT10Y", get = "economic.data") %>% 
    na.omit() %>% 
    tq_transmute(mutate_fun = to.period,
                 period     = "months", 
                 col_rename = "real_rate",
                 indexAt="firstof")
head(real_rate)
```

```{r}
dxy <- tq_get("DX-Y.NYB") %>% 
  na.omit() %>% 
  tq_transmute(select = close,
                    mutate_fun = to.monthly,
                    col_rename = "dxy.monthly",
                    indexAt="firstof")
head(dxy)
```
section note:
There is missing value in DXY so we need to deal with it using na.omit() and after first step cleaning each data frame will have no N/A value.

# Explorarotory Data Analysis

It's well known large sample should more or less follow Normal(Guassian) distribution. so we plot histagram of gld(23970 obs)

```{r distribution of gld}
ggplot(gld,aes(x=gld.monthly))+
  geom_histogram(bins = 50, color = "grey")+
  labs(title = "Histogram of gld price at close") +
  theme(legend.position = "none")
descdist(gld$gld.monthly, discrete = FALSE)
```

Clearly the gold etf price at market close is more $\beta$ distributed than normal distributed.

Now we explore the correlation between predictors to make sure they are not identical 
And because they differs in size, I will pick the the data from year 2012. it also serves as training data set

```{r cov between variables}
df_train <- data.frame(gld$date[1:96],
                 gld$gld.monthly[1:96],
                 UR$unemployment.rate[1:96],
                 us10y$US10.yield[1:96],
                 dxy$dxy.monthly[1:96],
                 real_rate$real_rate[1:96],
                 total_asset$total.asset[1:96])  
df_train <- rename(df_train,
       date=gld.date.1.96.,
       gld_close=gld.gld.monthly.1.96.,
       unemployt_rate=UR.unemployment.rate.1.96.,
       US10_yield=us10y.US10.yield.1.96.,
       DXY=dxy.dxy.monthly.1.96.,
       Real_rate=real_rate.real_rate.1.96.,
       total_asset=total_asset.total.asset.1.96.)

cor_df <- df_train %>%
  select_if(is.numeric) %>%
  lsr::correlate()

#rplot(cor_df$correlation)

melt_cor <- cor_df$correlation %>% 
  melt()

ggplot(melt_cor,aes(x=Var1, y=Var2, fill=value)) +
  geom_tile()+
  geom_text(aes(label = as.character(fashion(value))))
```

The selected predictor are not highly correlated except between the unemployment rate and gold close price, only 62% correlated.


```{r}
gldd <- diff(gld$gld.monthly)
plot.ts(gld$gld.monthly)
fit <- lm(gldd ~ as.numeric(1:length(gldd)))
abline(fit, col="red")
abline(h=mean(gld$gld.monthly), col="blue")
```

We can see clearly there is no trend on the differenced data. We can preceed with the current data without differencing.

```{r}
p1 <- ggplot(data = df_train,aes(x = date))+
  geom_line(aes(y = gld_close), color = "blue") +
  geom_line(aes(y = unemployt_rate*20), color = "red") +
  scale_y_continuous(
    # Features of the first axis
    name = "Price",
    # Add a second axis and specify its features
    sec.axis = sec_axis(trans = ~.*0.05, name="unemployment.rate")
  ) 
p2 <- ggplot(data = df_train,aes(x = date))+
  geom_line(aes(y = gld_close), color = "blue") +
  geom_line(aes(y = DXY), color = "red") +
  scale_y_continuous(
    # Features of the first axis
    name = "Price",
    # Add a second axis and specify its features
    sec.axis = sec_axis(trans = ~., name="Dollar Index")
  ) 
p3 <- ggplot(data = df_train,aes(x = date))+
  geom_line(aes(y = gld_close), color = "blue") +
  geom_line(aes(y = total_asset*0.00005), color = "red") +
  scale_y_continuous(
    # Features of the first axis
    name = "Price",
    # Add a second axis and specify its features
    sec.axis = sec_axis(trans = ~., name="Fed Total Asset")
  )
p4 <- ggplot(data = df_train,aes(x = date))+
  geom_line(aes(y = gld_close), color = "blue") +
  geom_line(aes(y = Real_rate*150), color = "red") +
  scale_y_continuous(
    # Features of the first axis
    name = "Price",
    # Add a second axis and specify its features
    sec.axis = sec_axis(trans = ~., name="US10Y real yeild rate")
  )

p1+p2+p3+p4
```

```{r}
P5 <- df_train %>%
  ggplot(aes(x=date, y=gld_close)) + 
  geom_point(alpha = 0.2) + 
  labs(title = 'scatterplot of GLD Price vs Year') +
  geom_smooth(method = 'lm', formula = 'y ~ x') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

P6 <- df_train %>%
  ggplot(aes(x=DXY, y=gld_close)) + 
  geom_point(alpha = 0.2) + 
  labs(title = 'scatterplot of GLD Price vs Dollar Index') +
  geom_smooth(method = 'lm', formula = 'y ~ x') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
P7 <- df_train %>%
  ggplot(aes(x=Real_rate, y=gld_close)) + 
  geom_point(alpha = 0.2) + 
  labs(title = 'scatterplot of GLD Price vs Real Rate') +
  geom_smooth(method = 'lm', formula = 'y ~ x') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
P8 <- df_train %>%
  ggplot(aes(x=total_asset, y=gld_close)) + 
  geom_point(alpha = 0.2) + 
  labs(title = 'scatterplot of GLD Price vs Total Aesst') +
  geom_smooth(method = 'lm', formula = 'y ~ x') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
P5+P6+P7+P8
```

## summary from EDA

-Suprisily there is a negative relationship between gold spot price and Federal reserve total asset.   
-there is divergence in correlation between gold price and unemployment rate since year 2016, which Trump published a basket of polices to boost economic activity includes not not limited to $print\ more\ money$. This information can be extracted   from Gold price v.s DXY plot, Dollar strength and gold price is $inversely$ correlated since 2016.  
-There is little correlation between real yield rate and gold price , this contraindicates the common knowledge of real rate $inversely$ correlated to gold price.  

# Model fitting

## Data Spilting 

Because COVID outbreak in 2020, so everything before year 2020 follows a different mechinsm than recent two years, the model prediction could show us how our life would be without COVID outbreak.

```{r}
set.seed(123)
GLD_train <- data.frame(gld$date[1:130],
                 gld$gld.monthly[1:130],
                 UR$unemployment.rate[1:130],
                 us10y$US10.yield[1:130],
                 dxy$dxy.monthly[1:130],
                 real_rate$real_rate[1:130],
                 total_asset$total.asset[1:130]) 
GLD_train <- rename(GLD_train,
       date=gld.date.1.130.,
       gld_close=gld.gld.monthly.1.130.,
       unemployt_rate=UR.unemployment.rate.1.130.,
       US10_yield=us10y.US10.yield.1.130.,
       DXY=dxy.dxy.monthly.1.130.,
       Real_rate=real_rate.real_rate.1.130.,
       total_asset=total_asset.total.asset.1.130.)

GLD_split <- GLD_train %>%
  initial_split(prop = 0.74) #it roughly seperates the data before and after year 2020
gld_train <- training(GLD_split)
gld_test <- testing(GLD_split)


```

## Split Checkiing
```{r}
dim(GLD_train)
dim(gld_train)
dim(gld_test)
a <- nrow(GLD_train)
# the number of observations for training data
b <- nrow(gld_train)
# the number of observations for test data
c <- nrow(gld_test)
# the percentage of observations for training data
per_train <- b/a
print(paste('the percentage of observations for training data is', per_train))
```

Training set include 96 observations and testing include 34 observations.

The percentage of training data observations is 0.738, which is almost equal to prob of 0.74, so the training and testing data sets have the appropriate number of observations.

For cross validation, we will use the caret package to achieve cross validation in model training.

## recipe creation& K-fold
```{r}
gld_recipe <- recipe(gld_close ~ unemployt_rate + US10_yield + DXY+
                      Real_rate + total_asset,
                    data = gld_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_zv(all_nominal_predictors())
               
gld_folds <- vfold_cv(gld_train, v = 10, repeats = 5)
```

## Ridge Regression
```{r}
#set up model
ridge_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

#set up workflow
ridge_workflow <- workflow() %>%
  add_recipe(gld_recipe) %>%
  add_model(ridge_spec)

# Create a regular grid
penalty_grid <- grid_regular(penalty(range = c(-5, 5), trans = log10_trans()), levels = 20)


# Fit the models to the folded data using tune_grid().
tune_res <- tune_grid(
  ridge_workflow,
  resamples = gld_folds,
  grid = penalty_grid
)



# use autoplot() on the results
autoplot(tune_res)
```

## collect Ridge metrics
```{r}
Ridge_RMSE <- collect_metrics(tune_res) %>%
  dplyr::select(.metric, mean, std_err)
Ridge_RMSE <-  Ridge_RMSE[c(1,2),]
```

Fit best Ridge model based on best $R^2$ then get result
```{r}
best_penalty <- select_best(tune_res, metric = "rsq")
best_penalty
```
```{r}
ridge_final <- finalize_workflow(ridge_workflow, best_penalty)
ridge_final_fit <- fit(ridge_final, data = gld_train)

Ridge_Prediction <- predict(ridge_final_fit, 
                            new_data = gld_test %>% 
                              dplyr::select(-`gld_close`))
Ridge_Prediction <- bind_cols(Ridge_Prediction, 
                              gld_test %>% 
                                dplyr::select(`gld_close`))

Ridge_Graph <- Ridge_Prediction %>%
  ggplot(aes(x=.pred, y=`gld_close`)) + 
  geom_point(alpha = 1) + 
  geom_abline(lty = 2) + 
  theme_bw() + 
  coord_obs_pred()

Ridge_Accuracy <- augment(ridge_final_fit, new_data = gld_test) %>%
  rsq(truth = `gld_close`, estimate = .pred)
```

## Lasso Regression
```{r}
lasso_spec <-
  linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")


lasso_workflow <- workflow() %>%
  add_recipe(gld_recipe) %>%
  add_model(lasso_spec)



tune_res_lasso <- tune_grid(
  lasso_workflow,
  resamples = gld_folds,
  grid = penalty_grid
)


autoplot(tune_res_lasso)
```

```{r}
Lasso_RMSE <- collect_metrics(tune_res_lasso) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head(2)#?
```

collect metrics of LASSO regression and select the best mdoel based on $R^2$

```{r}
best_penalty_lasso <- select_best(tune_res_lasso, metric = "rsq")
lasso_final <- finalize_workflow(lasso_workflow, best_penalty_lasso)

lasso_final_fit <- fit(lasso_final, data = gld_train)
Lasso_Prediction <- predict(lasso_final_fit, new_data = gld_test %>% 
                              dplyr::select(-`gld_close`))
Lasso_Prediction <- bind_cols(Lasso_Prediction, gld_test %>% 
                                dplyr::select(`gld_close`))
Lasso_Graph <- Lasso_Prediction %>%
  ggplot(aes(x=.pred, y=`gld_close`)) + 
  geom_point(alpha=1) + 
  geom_abline(lty = 2) + 
  theme_bw() + 
  coord_obs_pred()
Lasso_Accuracy <- augment(lasso_final_fit, new_data = gld_test) %>%
  rsq(truth = `gld_close`, estimate = .pred)
```


## KNN
```{r}

KNN_spec <-nearest_neighbor() %>%
  set_mode("regression") %>%
  set_engine("kknn") %>% 
  set_args(neighbors = tune(),
           weight_func = tune(),
           dist_power = tune())

translate(KNN_spec)

KNN_workflow <- workflow() %>%
  add_recipe(gld_recipe) %>%
  add_model(KNN_spec)


knn1_res <- tune_grid(
  KNN_workflow,
  resamples = gld_folds,
  
)

autoplot(knn1_res)

KNN_RMSE <- collect_metrics(knn1_res) %>% 
  dplyr::select(.metric, mean, std_err) %>% 
  head()


best_KNN_final <- select_best(knn1_res,metric = "rsq")
best_KNN_final_model <- finalize_workflow(KNN_workflow, best_KNN_final)

best_KNN_final_model_fit <- fit(best_KNN_final_model, data = gld_train)
KNN_Prediction <- predict(best_KNN_final_model_fit, 
                            new_data = gld_test %>% 
                              dplyr::select(-`gld_close`))
KNN_Prediction <- bind_cols(KNN_Prediction, 
                              gld_test %>% 
                              dplyr::select(`gld_close`))
KNN_Graph <- KNN_Prediction %>%
  ggplot(aes(x=.pred, y=`gld_close`)) + 
  geom_point(alpha=1) + 
  geom_abline(lty = 2) + 
  theme_bw() + 
  coord_obs_pred()
KNN_Accuracy <- augment(best_KNN_final_model_fit, new_data = gld_test) %>%
  rsq(truth = `gld_close`, estimate = .pred)
```

## Boosted model

```{r}
boost_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_wf <- workflow() %>%
  add_model(boost_spec %>%
  set_args(trees = tune())) %>%
  add_recipe(gld_recipe)

boost_grid <- grid_regular(trees(range = c(10, 100)), levels = 10)

boost_tune_res <- tune_grid(
  boost_wf,
  resamples = gld_folds,
  grid = boost_grid,
)
autoplot(boost_tune_res)
```

collect metrics and select best mdoel based on $R^2$

```{r}
Boost_RMSE <- collect_metrics(boost_tune_res) %>% 
  dplyr::select(.metric, mean, std_err) %>%
  head()
```

```{r}
best_boost_final <- select_best(boost_tune_res,metric = "rsq")
best_boost_final_model <- finalize_workflow(boost_wf, best_boost_final)

best_boost_final_model_fit <- fit(best_boost_final_model, data = gld_train)
Boost_Prediction <- predict(best_boost_final_model_fit, 
                            new_data = gld_test %>% 
                              dplyr::select(-`gld_close`))
Boost_Prediction <- bind_cols(Boost_Prediction, 
                              gld_test %>% 
                              dplyr::select(`gld_close`))
Boost_Graph <- Boost_Prediction %>%
  ggplot(aes(x=.pred, y=`gld_close`)) + 
  geom_point(alpha=1) + 
  geom_abline(lty = 2) + 
  theme_bw() + 
  coord_obs_pred()
Boost_Accuracy <- augment(best_boost_final_model_fit, new_data = gld_test) %>%
  rsq(truth = `gld_close`, estimate = .pred)
```

## Decision Tree model

```{r}
tree_spec <-decision_tree() %>%
  set_engine("rpart")
class_tree_spec <- tree_spec %>%
  set_mode("regression")
  
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% 
            set_args(cost_complexity = tune())) %>%
  add_recipe(gld_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-5, 0)), levels = 10)
tune_res_tree <- tune_grid(
  class_tree_wf,
  resamples = gld_folds,
  grid = param_grid,
)
autoplot(tune_res_tree)
```

```{r}
Tree_RMSE <- collect_metrics(tune_res_tree) %>%
  dplyr::select(.metric, mean, std_err) %>%
  head(2)
```

```{r}
library(rpart.plot)
best_complexity <- select_best(tune_res_tree, "rsq")
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)
class_tree_final_fit <- fit(class_tree_final, data = gld_train)
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

From the plot of the decision tree, the most important variables are: DXY. Now we predict the price on the test set.

```{r}
Tree_Prediction <- predict(class_tree_final_fit, 
                           new_data = gld_test %>% 
                             dplyr::select(-`gld_close`))
Tree_Prediction <- bind_cols(Tree_Prediction, 
                             gld_test %>% 
                               dplyr::select(`gld_close`))
Tree_Graph <- Tree_Prediction %>%
  ggplot(aes(x=.pred, y=`gld_close`)) + 
  geom_point(alpha=1) + 
  geom_abline(lty = 2) + 
  theme_bw() + 
  coord_obs_pred()
Tree_Accuracy <- augment(class_tree_final_fit, new_data = gld_test) %>%
  rsq(truth = `gld_close`, estimate = .pred)
```

## Model comparison

```{r}
library(ggpubr)
figure <- ggarrange(Ridge_Graph, Lasso_Graph, Boost_Graph,Tree_Graph,KNN_Graph,
                    labels = c("Ridge", "Lasso", "Boost","Tree","KNN"),
                    ncol = 3, nrow = 3,font.label = list(size = 11, color = "cadetblue2", face = "bold"))
figure
```


it's clear that Boost model has the points closest to the dotted line, which means it has the best performance in the four models.

# RMSE & RSQ in Training Set
```{r}
head(Ridge_RMSE)
head(Lasso_RMSE)
head(Boost_RMSE, 2)
head(Tree_RMSE, 2)
head(KNN_RMSE,2)
```

So the KNN model has the highest RSQ on the training set, decision tree has the smallest rmse on the training set. Based on the RSQ, boost model is the best.

# R-Squared of Testing Set
```{r}
rsq_comparisons <- bind_rows(Ridge_Accuracy, Lasso_Accuracy, Boost_Accuracy, Tree_Accuracy,KNN_Accuracy) %>% 
  tibble() %>% 
  mutate(model = c("Ridge", "Lasso", "Boost", "Tree","KNN")) %>% 
  dplyr::select(model, .estimate) %>%
  arrange(.estimate)
rsq_comparisons
```

Looking at the R-Squared of the test set. Of the 4 models, the Boost model has the highest R squared 0.929,  higher than the others.

# Conlcusion
From our exploratory data analysis and model fitting part, we can know that the KNN model performs much better than the other methods. Of all these methods, the LASSO regression perform the worst. Also we should have in mind that DXY is the most correlated predictors.
