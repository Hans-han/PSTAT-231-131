---
title: "Homework 2"
author: "PSTAT 131/231"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Linear Regression

For this lab, we will be working with a data set from the UCI (University of California, Irvine) Machine Learning repository ([see website here](http://archive.ics.uci.edu/ml/datasets/Abalone)). The full data set consists of $4,177$ observations of abalone in Tasmania. (Fun fact: [Tasmania](https://en.wikipedia.org/wiki/Tasmania "Tasmania") supplies about $25\%$ of the yearly world abalone harvest.)

![*Fig 1. Inside of an abalone shell.*](https://cdn.shopify.com/s/files/1/1198/8002/products/1d89434927bffb6fd1786c19c2d921fb_2000x_652a2391-5a0a-4f10-966c-f759dc08635c_1024x1024.jpg?v=1582320404){width="152"}

The age of an abalone is typically determined by cutting the shell open and counting the number of rings with a microscope. The purpose of this data set is to determine whether abalone age (**number of rings + 1.5**) can be accurately predicted using other, easier-to-obtain information about the abalone.

The full abalone data set is located in the `\data` subdirectory. Read it into *R* using `read_csv()`. Take a moment to read through the codebook (`abalone_codebook.txt`) and familiarize yourself with the variable definitions.

Make sure you load the `tidyverse` and `tidymodels`!

### Question 1

Your goal is to predict abalone age, which is calculated as the number of rings plus 1.5. Notice there currently is no `age` variable in the data set. Add `age` to the data set.

Assess and describe the distribution of `age`.

```{r}
library(tidymodels)
library(tidyverse)
aba <- read.csv("/Users/hanshan/Desktop/R_project/PSTAT-231-131/homework-2/data/abalone.csv")#read file
aba$age <- aba$ring+1.5# add variable

ggplot(data=aba)+
  geom_histogram(mapping =  aes(x = age))
curve(dchisq(x, df = 10), from = 0, to = 30)
# chi-square plot cite from https://www.statology.org/plot-chi-square-distribution-in-r/
```

From the plot of age variable above, it's clear age follows a $\chi^{2}$ distribution

### Question 2

Split the abalone data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data.

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

```{r}
set.seed(1)

aba_no_ring <- subset(aba, select = -c(rings)) #  exclude rings

split <- initial_split(aba_no_ring, prop = 0.8,strata = diameter) #80% train 20% test
aba_training <- training(split) # training data set
aba_testing <- testing(split) #  testing data set
```

### Question 3

Using the **training** data, create a recipe predicting the outcome variable, `age`, with all other predictor variables. Note that you should not include `rings` to predict `age`. Explain why you shouldn't use `rings` to predict `age`.

Steps for your recipe:

1.  dummy code any categorical predictors

2.  create interactions between

    -   `type` and `shucked_weight`,
    -   `longest_shell` and `diameter`,
    -   `shucked_weight` and `shell_weight`

3.  center all predictors, and

4.  scale all predictors.

You'll need to investigate the `tidymodels` documentation to find the appropriate step functions to use.

```{r}
Cat_dummy <- recipe(age ~ ., data = aba_training) %>% 
  step_dummy(all_nominal_predictors(),-all_outcomes()) %>% 
  step_center(all_nominal_predictors()) %>% 
  step_scale(all_nominal_predictors()) %>% 
  step_interact(terms = ~ starts_with("type"):shucked_weight) %>%   
  step_interact(terms = ~ longest_shell:diameter) %>%   
  step_interact(terms = ~ shucked_weight:shell_weight)# %>%

```

Because age = rings + 1.5

### Question 4

Create and store a linear regression object using the `"lm"` engine.

```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")# linear model selection
```

### Question 5

Now:

1.  set up an empty workflow,
2.  add the model you created in Question 4, and
3.  add the recipe that you created in Question 3.

```{r}
wkfl <- workflow() %>%
   add_model(lm_model) %>% 
   add_recipe(Cat_dummy) 
wkfl   
```

### Question 6

Use your `fit()` object to predict the age of a hypothetical female abalone with longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, shell_weight = 1.

```{r}
type <- c("F")
longest_shell <- c(0.50)
diameter <-  c(0.10)
height <-  c(0.30 )
whole_weight <-  c(4) 
shucked_weight <-  c(1) 
viscera_weight <-  c(2)
shell_weight <-  c(1)
test_case <- data.frame(type,longest_shell,diameter,height,whole_weight,shucked_weight,viscera_weight,shell_weight)

lm_fit <- fit(wkfl,aba_training) 
        
predict(lm_fit,test_case)
#lm_fit
```

### Question 7

Now you want to assess your model's performance. To do this, use the `yardstick` package:

1.  Create a metric set that includes *R^2^*, RMSE (root mean squared error), and MAE (mean absolute error).
2.  Use `predict()` and `bind_cols()` to create a tibble of your model's predicted values from the **training data** along with the actual observed ages (these are needed to assess your model's performance).
3.  Finally, apply your metric set to the tibble, report the results, and interpret the *R^2^* value.

```{r}
library(yardstick)
aba_metrics <- metric_set(rmse, rsq, mae)
aba_train_res <- predict(lm_fit,aba_training %>% select(-age))
#aba_train_res
aba_test_res <- bind_cols(aba_train_res, aba_training %>% select(age))
#aba_test_res
aba_metrics(aba_test_res, truth = age, estimate = .pred)


```

### Required for 231 Students

In lecture, we presented the general bias-variance tradeoff, which takes the form:

$$
E[(y_0 - \hat{f}(x_0))^2]=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
$$

where the underlying model $Y=f(X)+\epsilon$ satisfies the following:

-   $\epsilon$ is a zero-mean random noise term and $X$ is non-random (all randomness in $Y$ comes from $\epsilon$);
-   $(x_0, y_0)$ represents a test observation, independent of the training set, drawn from the same model;
-   $\hat{f}(.)$ is the estimate of $f$ obtained from the training set.

#### Question 8

Which term(s) in the bias-variance tradeoff above represent the reproducible error? Which term(s) represent the irreducible error?

$Var(\hat{f}(x_0))$ is the reproducible error . $Var(\epsilon)$ is irreducible error

#### Question 9

Using the bias-variance tradeoff above, demonstrate that the expected test error is always at least as large as the irreducible error.

If we take $\hat{f}(x_0)=E[Y|X=x_0]$ which minimize Var($\hat{f}(x_0)$)+$[Bias(\hat{f}(x_0))]^2$ to zero, then the only term left is $Var(\epsilon)$

#### Question 10

Prove the bias-variance tradeoff.

Hints:

-   use the definition of $Bias(\hat{f}(x_0))=E[\hat{f}(x_0)]-f(x_0)$;
-   reorganize terms in the expected test error by adding and subtracting $E[\hat{f}(x_0)]$


\begin{aligned}
Err(x_{0})   &= E[Y-\hat{f}(x_0)|X = x_0]\\
             &= \sigma_{\epsilon}^{2}+[E\hat{f}(x_0)-f(x_0)]^{2}+E[\hat{f}(x_0)-E\hat{f}(x_0)]^{2}\\ 
             &= \sigma_{\epsilon}^{2}+Bias^{2}(\hat{f}(x_0))+Var(\hat{f}(x_0)) \\ 
             &= Irreducible\ Error + Bias^{2} + Variance 
\end{aligned}



Expansion below:

             
\begin{aligned}
E\left[\left(y_0-\hat{f}\left(x_0\right)^2\right]\right.&=E\left[\left(f\left(x_0\right)+\epsilon+\hat{f}\left(x_0\right)\right)^2\right] \\
&=E\left[f\left(x_0\right)-\hat{f}\left(x_0\right)^2\right]+E\left[\epsilon^2\right]+2 E\left[\left(f\left(x_0\right)-\hat{f}\left(x_0\right)\right)\right] E[\epsilon] \\
&=E\left[f\left(x_0\right)-\hat{f}\left(x_0\right)^2\right]+\operatorname{Var}(\epsilon)
\end{aligned}







\begin{aligned}
E\left[f\left(x_0\right)-\hat{f}\left(x_0\right)^2\right] &=E\left[\left(\left(f\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)-\left(\hat{f}\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)\right)^2\right] \\
&\left.=E\left[\left(E\left[\hat{f}\left(x_0\right)\right]-f\left(x_0\right)\right)^2\right]+E\left[\hat{f}\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)^2\right] \\
&-2 E\left[\left(f\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)\left(\hat{f}\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)\right] \\
&\left.=\left(E\left[\hat{f}\left(x_0\right)\right]-f\left(x_0\right)\right)^2+E\left[\hat{f}\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)^2\right] \\
&-2\left(f\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right) E\left[\left(\hat{f}\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)\right] \\
&\left.=\left(E\left[\hat{f}\left(x_0\right)\right]-f\left(x_0\right)\right)^2+E\left[\hat{f}\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)^2\right] \\
&-2\left(f\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)\left(E\left[\hat{f}\left(x_0\right)\right]-E\left[\hat{f}\left(x_0\right)\right]\right) \\
&\left.=\left(E\left[\hat{f}\left(x_0\right)\right]-f\left(x_0\right)\right)^2+E\left[\hat{f}\left(x_0\right)-E\left[\hat{f}\left(x_0\right)\right]\right)^2\right] \\
&=\left[\operatorname{Bias}\left(\hat{f}\left(x_0\right)\right)\right]^2+\operatorname{Var}\left(\hat{f}\left(x_0\right)\right)
\end{aligned}


